{"cells":[{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C84DE0B0E2724712BFCACBAB1836D7E7","mdEditEnable":false},"source":"# 推荐评论展示任务"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"5401031CE58D430694CC694E8888724F","mdEditEnable":false},"source":"# 任务描述 \n\n本次推荐评论展示任务的目标是从真实的用户评论中，挖掘合适作为推荐理由的短句。点评软件展示的推荐理由具有长度限制，而真实用户评论语言通顺、信息完整。综合来说，两者都具有用户情感的正负向，但是展示推荐理由的内容相关性高于评论，需要较强的文本吸引力。一些真实的推荐理由如下图所示：\n\n![推荐理由](https://boyuai.oss-cn-shanghai.aliyuncs.com/disk/YouthAI%E7%A7%8B%E5%AD%A3%E6%80%9D%E7%BB%B4%E7%8F%AD-%E4%B8%8A%E8%AF%BE%E8%A7%86%E9%A2%91/textcls_pic/apppic.png)\n\n"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4BC380A844734755894CBEA0DB2EE98A"},"source":"# 数据集\n\n本次推荐评论展示任务所采用的数据集是点评软件中，用户中文评论的集合。\n\n\n## 数据样例\n本次任务要求将这些评论分为两类，即“展示”和“不展示”，分别以数字1和0作为标注，如下图所示：\n\n\n![数据样例](https://boyuai.oss-cn-shanghai.aliyuncs.com/disk/YouthAI%E7%A7%8B%E5%AD%A3%E6%80%9D%E7%BB%B4%E7%8F%AD-%E4%B8%8A%E8%AF%BE%E8%A7%86%E9%A2%91/textcls_pic/dataexp.png)\n\n## 文档说明 \n\n\n数据集文件分为训练集和测试集部分，对应文件如下：\n\n- 带标签的训练数据：`train_shuffle.txt` \n- 不带标签的测试数据：`test_handout.txt`\n\n注意，`test_handout.txt`文件的行索引从0开始，对应于ID一列，评论内容为“展示”的预测概率应于Prediction一列。\n\n需要注意的是，由于数据在标注时存在主观偏好，标记为“不展示”（0）的评论不一定是真正的负面评论，反之亦然。但是这种情况的存在，不会对任务造成很大的歧义，通过基准算法我们可以在测试集上实现很高的性能。"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"917C94A1B0A840A08281F2487B78C8F2"},"source":"# 任务拆解\n\n推荐评论展示任务需要对数据集中的评论文本文件进行处理，传统的学习方式流程为：\n\n![学习流程](https://boyuai.oss-cn-shanghai.aliyuncs.com/disk/YouthAI%E7%A7%8B%E5%AD%A3%E6%80%9D%E7%BB%B4%E7%8F%AD-%E4%B8%8A%E8%AF%BE%E8%A7%86%E9%A2%91/textcls_pic/method.png)\n\n对于以上的任务，近年来兴起的深度学习算法都可以实现：\n\n![学习流程](https://boyuai.oss-cn-shanghai.aliyuncs.com/disk/YouthAI%E7%A7%8B%E5%AD%A3%E6%80%9D%E7%BB%B4%E7%8F%AD-%E4%B8%8A%E8%AF%BE%E8%A7%86%E9%A2%91/textcls_pic/deepmethod.png)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7E7BEA9BC238421E8B0D55A60C7608CB"},"source":"# 评估说明\n\n## 评价指标\n\n本次任务采用 [AUC（Area Under Curve)](https://baike.baidu.com/item/AUC/19282953) 作为模型的评价标准。\n\n## 在线评估\n\n评估函数首先会验证选手提交的预测结果文件是否符合要求，主要验证了以下要求:\n\n1. 提交的预测文件是否存在重复ID\n2. 提交的预测文件ID是否与测试集文件ID不匹配\n3. 提交的预测文件Prediction列是否存在整数（auc测评要求选手提供的是概率值，而非分类结果0或1）\n\n通过验证后的文件会用以AUC为测评指标的函数进行计算评估。\n\n\n## 文件格式\n\n由于测评脚本已经统一，为保证脚本的顺利运行，在进行测评时，要求选手提交的`预测文件`拥有规范的字段名和字段格式，预测文件具体要求如下：\n\n| NO | 字段名称 | 数据类型 | 字段描述 |\n| -------- | -------- | -------- | -------- |\n| 1    | ID     | int    | ID序列     |\n| 2    | Prediction   | float     | 预测结果（概率值）   |\n\n正确格式的提交文件样例: `submission_random.csv`。\n\n## 基准算法\n\n本次任务采用不同的基准算法，获得模型的AUC如下：\n- 随机基准算法AUC：0.51209\n- 弱基准算法AUC：0.85107\n- 强基准算法AUC：0.94452\n在评估时，以弱基准算法的AUC作为达标线。\n"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"51BE6D7E72D5436FAC6C2A39AFAA6D4D","mdEditEnable":false},"source":"## 终审评估\n\n本次任务的终审评估将挑选在评分指标位于前10名的同学进行项目报告撰写，以描述模型、算法及实验等相关内容和结果，报告排版要求届时发布。\n\n除此以外，为保证竞赛的公平性，进入终审评估的同学需要提交项目代码，由助教进行模型的有效性验证。\n\n如发现实验结果有较大差异，或者模型无法复现等问题，组委会将取消营员本次14天陪你挑战《动手学深度学习》的结营资格，并且进行公示。"},{"metadata":{"id":"1A429BC3EA8A42B789DC1026C1C800C6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Requirement already satisfied: torchtext in /opt/conda/lib/python3.6/site-packages\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torchtext)\nRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from torchtext)\nRequirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from torchtext)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from torchtext)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from torchtext)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchtext)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->torchtext)\nRequirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->torchtext)\nRequirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->torchtext)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->torchtext)\n\u001b[33mYou are using pip version 9.0.1, however version 20.0.2 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","name":"stdout"}],"source":"pip install torchtext","execution_count":1},{"metadata":{"id":"B531ED31FCF5466D8E42F62BD1F3869F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import collections\nimport os\nimport random\nimport time\nfrom tqdm import tqdm\nimport torch\nfrom torch import nn\nimport torchtext.vocab as Vocab\nimport torch.utils.data as Data\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch import nn, optim\nimport sys\nimport math\nimport re\nimport pandas as pd\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":2},{"metadata":{"id":"C603CCEDFBBE4B58809D94DAB18A76F9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def read_comments_trian():\n    with open('/home/kesci/work/Comments9120/train_shuffle.txt') as f:\n        tags = [re.sub('[^0-1]+', ' ', line.strip()) for line in f]\n    with open('/home/kesci/work/Comments9120/train_shuffle.txt') as f:\n        lines = [line.strip().replace('0', '').replace('1', '').replace('\\t', '') for line in f]\n        line_nums=range(0,len(lines))\n    return tags,lines,line_nums\n\n#train_shuffle\ntags,lines,line_nums = read_comments_trian()\ntrain_data=[]\nfor i in range(0,len(tags)):\n    train_data.append([lines[i],int(tags[i])])","execution_count":3},{"metadata":{"id":"0897352159FC4ACE890D8DE2BF6FB65E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def get_tokenized_imdb(data):\n    '''\n    @params:\n        data: 数据的列表，列表中的每个元素为 [文本字符串，0/1标签] 二元组\n    @return: 切分词后的文本的列表，列表中的每个元素为切分后的词序列\n    '''\n    back=[]\n    for line in data:\n        tempt=[]\n        for word in line[0]:\n            tempt.append(word)\n        back.append(tempt)\n    return back\ndef get_vocab_imdb(data):\n    '''\n    @params:\n        data: 同上\n    @return: 数据集上的词典，Vocab 的实例（freqs, stoi, itos）\n    '''\n    tokenized_data = get_tokenized_imdb(data)\n    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n    return Vocab.Vocab(counter, min_freq=0)\n\nvocab = get_vocab_imdb(train_data)","execution_count":4},{"metadata":{"id":"0F517BB5A95C4DF3BA85FBD9209BBDF9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def preprocess_imdb(data, vocab):\n    '''\n    @params:\n        data: 同上，原始的读入数据\n        vocab: 训练集上生成的词典\n    @return:\n        features: 单词下标序列，形状为 (n, max_l) 的整数张量\n        labels: 情感标签，形状为 (n,) 的0/1整数张量\n    '''\n    max_l = 500  # 将每条评论通过截断或者补0，使得长度变成500\n\n    def pad(x):\n        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n\n    tokenized_data = get_tokenized_imdb(data)\n    features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n    labels = torch.tensor([score for _, score in data])\n    return features, labels\ntrain_set = Data.TensorDataset(*preprocess_imdb(train_data, vocab))\nbatch_size = 64\ntrain_iter = Data.DataLoader(train_set, batch_size, shuffle=True)","execution_count":5},{"metadata":{"id":"20BFF6B31B2F466CB5C8A3A2F9D14067","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"class BiRNN(nn.Module):\n    def __init__(self, vocab, embed_size, num_hiddens, num_layers):\n        '''\n        @params:\n            vocab: 在数据集上创建的词典，用于获取词典大小\n            embed_size: 嵌入维度大小\n            num_hiddens: 隐藏状态维度大小\n            num_layers: 隐藏层个数\n        '''\n        super(BiRNN, self).__init__()\n        self.embedding = nn.Embedding(len(vocab), embed_size)\n        \n        # encoder-decoder framework\n        # bidirectional设为True即得到双向循环神经网络\n        self.encoder = nn.LSTM(input_size=embed_size, \n                                hidden_size=num_hiddens, \n                                num_layers=num_layers,\n                                bidirectional=True)\n        self.decoder = nn.Linear(4*num_hiddens, 2) # 初始时间步和最终时间步的隐藏状态作为全连接层输入\n        \n    def forward(self, inputs):\n        '''\n        @params:\n            inputs: 词语下标序列，形状为 (batch_size, seq_len) 的整数张量\n        @return:\n            outs: 对文本情感的预测，形状为 (batch_size, 2) 的张量\n        '''\n        # 因为LSTM需要将序列长度(seq_len)作为第一维，所以需要将输入转置\n        embeddings = self.embedding(inputs.permute(1, 0)) # (seq_len, batch_size, d)\n        # rnn.LSTM 返回输出、隐藏状态和记忆单元，格式如 outputs, (h, c)\n        outputs, _ = self.encoder(embeddings) # (seq_len, batch_size, 2*h)\n        encoding = torch.cat((outputs[0], outputs[-1]), -1) # (batch_size, 4*h)\n        outs = self.decoder(encoding) # (batch_size, 2)\n        return outs","execution_count":6},{"metadata":{"id":"826BD274CA154E9996AADAF3C2F57D0E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"There are 2475 oov words.\n","name":"stdout"}],"source":"embed_size, num_hiddens, num_layers = 100, 100, 2\nnet = BiRNN(vocab, embed_size, num_hiddens, num_layers)\ncache_dir = \"/home/kesci/input/GloVe6B5429\"\nglove_vocab = Vocab.GloVe(name='6B', dim=100, cache=cache_dir)\ndef load_pretrained_embedding(words, pretrained_vocab):\n    '''\n    @params:\n        words: 需要加载词向量的词语列表，以 itos (index to string) 的词典形式给出\n        pretrained_vocab: 预训练词向量\n    @return:\n        embed: 加载到的词向量\n    '''\n    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0]) # 初始化为0\n    oov_count = 0 # out of vocabulary\n    for i, word in enumerate(words):\n        try:\n            idx = pretrained_vocab.stoi[word]\n            embed[i, :] = pretrained_vocab.vectors[idx]\n        except KeyError:\n            oov_count += 1\n    if oov_count > 0:\n        print(\"There are %d oov words.\" % oov_count)\n    return embed\nnet.embedding.weight.data.copy_(load_pretrained_embedding(vocab.itos, glove_vocab))\nnet.embedding.weight.requires_grad = True # 词向量不匹配需要更新","execution_count":7},{"metadata":{"id":"1AF2DE93BD204F9C89EE66D23C8ED173","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"training on  cuda\nepoch 1, loss 0.3839, train acc 0.834,  time 67.8 sec\nepoch 2, loss 0.1246, train acc 0.902,  time 67.6 sec\nepoch 3, loss 0.0716, train acc 0.916,  time 67.7 sec\nepoch 4, loss 0.0473, train acc 0.926,  time 67.6 sec\nepoch 5, loss 0.0342, train acc 0.937,  time 67.7 sec\n","name":"stdout"}],"source":"def evaluate_accuracy(data_iter, net, device=None):\n    if device is None and isinstance(net, torch.nn.Module):\n        device = list(net.parameters())[0].device \n    acc_sum, n = 0.0, 0\n    with torch.no_grad():\n        for X, y in data_iter:\n            if isinstance(net, torch.nn.Module):\n                net.eval()\n                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n                net.train()\n            else:\n                if('is_training' in net.__code__.co_varnames):\n                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n                else:\n                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n            n += y.shape[0]\n    return acc_sum / n\n\ndef train(train_iter,  net, loss, optimizer, device, num_epochs):\n    net = net.to(device)\n    print(\"training on \", device)\n    batch_count = 0\n    for epoch in range(num_epochs):\n        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n        for X, y in train_iter:\n            X = X.to(device)\n            y = y.to(device)\n            y_hat = net(X)\n            l = loss(y_hat, y) \n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n            train_l_sum += l.cpu().item()\n            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n            n += y.shape[0]\n            batch_count += 1\n        #test_acc = evaluate_accuracy(test_iter, net)\n        print('epoch %d, loss %.4f, train acc %.3f,  time %.1f sec'\n              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, time.time() - start))\nlr, num_epochs = 0.01, 5\noptimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\nloss = nn.CrossEntropyLoss()\ntrain(train_iter, net, loss, optimizer, device, num_epochs)","execution_count":8},{"metadata":{"id":"3216671DA93947DCB5C724F5CEEFDD47","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def predict_sentiment(net, vocab, sentence):\n    '''\n    @params：\n        net: 训练好的模型\n        vocab: 在该数据集上创建的词典，用于将给定的单词序转换为单词下标的序列，从而输入模型\n        sentence: 需要分析情感的文本，以单词序列的形式给出\n    @return: 预测的结果，positive 为正面情绪文本，negative 为负面情绪文本\n    '''\n    device = list(net.parameters())[0].device # 读取模型所在的环境\n    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\n    label = net(sentence.view((1, -1)))\n    tempt0=label[0][0].item()\n    tempt1=label[0][1].item()\n    result1=np.exp(tempt1)/(np.exp(tempt0)+np.exp(tempt1))\n    return result1\n","execution_count":9},{"metadata":{"id":"08AB6CB322E04C488FA99891CE4246E3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def read_comments_test():\n    with open('/home/kesci/work/Comments9120/test_handout.txt') as f:\n        test_item= [line.strip().replace('0', '').replace('1', '') for line in f]\n        test_nums=range(0,len(lines))\n    return test_item,test_nums\ntest_item,test_nums=read_comments_test()","execution_count":10},{"metadata":{"id":"EE88C048047C49EA8FA6D5B9717657E0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"test_set=[]\nfor line in test_item:\n    tempt=[]\n    for word in line:\n        tempt.append(word)\n    test_set.append(tempt)\nprob=[]\nfor test_text in test_set:\n    tempt=predict_sentiment(net, vocab, test_text)\n    prob.append(tempt)\nresult=[]\nfor i in range(len(prob)):\n    result.append([test_nums[i],prob[i]])\ncolumn=['ID','Prediction']  \ntest=pd.DataFrame(columns=column,data=result)\ntest.to_csv('/home/kesci/work/test_submission.csv')","execution_count":11},{"metadata":{"id":"07E96C43B9FC449EAE699EDA076B0AAD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}